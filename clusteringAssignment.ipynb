{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is an unsupervised learning approach? Why is it needed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An unsupervised learning approach is a part of machine learning where algorithms are used on data without the dependent variable. These methods are needed when we want to extract certain information from data and we only have information about features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is an unsupervised learning algorithm where we want to group our data into clusters based on their similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tHow do clustering and classification differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference is that classification is supervised and clustering an unsupervised learning approach. This means that in classification we will have predefined classes (dependent variable) and with the clustering that is not the case. Here we do not know how clusters will look alike and usually even how many clusters there will be (we are looking for the optimal number)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat are the various applications of clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering can be used for:\n",
    "\n",
    "- grouping data (for example grouping customers based on similarity)\n",
    "- handling outliers (also handling if there is some ill action)\n",
    "- in semisupervised learning, when we want to group data before applying supervised algorithms\n",
    "- in dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tHow does clustering play a role in supervised learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, clustering is useful in handling the data before applying supervised algorithms. We can apply this unsupervised learning approach for dimensionality reduction, finding outliers, and more. Also, we can use clustering for grouping data and later on we could apply different supervised algorithms on different clusters. The reason behind this is that one algorithm maybe wouldn't work on the whole dataset. Therefore, by grouping based on their similarity, we have a better chance to find some patterns in each cluster separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are the requirements to be met by a clustering algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering algorithm should be:\n",
    "   \n",
    "\n",
    "- scalable\n",
    "- able to work with different shapes of data\n",
    "- able to work with both continuous and categorical data\n",
    "- able to work with a large number of features\n",
    "- able to handle outliers\n",
    "- not dependable on the number of records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tDiscuss the different approaches for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two approaches for clustering: agglomerative (bottom-up) and divisive (top-down). In the agglomerative approach, we start from the point where each data is its own cluster. Then, based on similarity, we group our data until there is only one cluster. Finding the optimal value of clusters is basically deciding when to stop. On the other hand, in divisive approach, we start from the point where all data are in one cluster and we divide them until each data is one cluster.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is WCSS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WCSS stands for within-cluster sumbansion of squares. It is the sum of squared distances between all data in one cluster. Our goal is to have the lowest WCSS, however, we must take into consideration the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tDiscuss the elbow method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the elbow method to find the optimal value for some parameter. In the case of K-means, we are finding the optimal value for k-number of clusters. With the increase of k, variance inside the cluster is decreasing (we are having less and less data inside the cluster), however besides decreasing the inner, we would like to have a high enough outer variance. Elbow method provides us with the elbow resampled graph showing where the increasing value of k doesn't result in a high decrease of inner variance. The first value where it happens is the optimal one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhat is the significance of ‘K’ in K-Means and how is it calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K is the number of clusters and, as said above, with the increase of k, we are decreasing the number of elements in the cluster which as a result have lower inner variance. Finding optimal value for k is crucial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tDiscuss the step by step implementation of K-Means Clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take two arbitrary points to be starting centroids\n",
    "\n",
    "- Calculate the distance between them and the rest of the data. Take the closest points to each centroid. \n",
    "\n",
    "- Next centroid is located on the half distance between the starting centroid and the closest point. These are the starting clusters.\n",
    "\n",
    "- Repeat the process for the rest of the data\n",
    "\n",
    "- Calculate the wcss for each cluster\n",
    "\n",
    "- Repeat the process fo k>2 and find the optimal one k'\n",
    "\n",
    "- Repeat the process for k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tWhat are the challenges with K-Means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest challenges in K-Means is deciding on the number of clusters or value of K. Furthermore, when selecting arbitrary points, we can select too close points which will not result in the optimal grouping. One more challenge is that we need to repeat the process in order to arrive to the optimal clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tDiscuss the various improvements in K-Means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- K++ - uses distant centroids in order to come to the optimal solution\n",
    "- use mini batches of tha data - this speeds up the process as centroids move slightly\n",
    "- using triangle rule: if a, b and c are sides of the triangle, a+b>c. As a result, we do not have to calculate all distances between points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Clustering:\n",
    "14.\tDiscuss the agglomerative and divisive clustering approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I am not wrong, this is the same question as the question 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tWhat are dendrograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dendrograms are graphs that show us the way our data is grouped. The purpose of these graphs is to find the optimal number of clusters in hierarchical clustering. The larger the linkage is, the similarity between data is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tDiscuss the Hierarchical clustering in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical clustering is easy to interpret and understand. First, we need to know is that the Hierarchical uses an agglomerative approach which means that each point is the separate clusters. Next, we calculate the distance and group data into pairs based on the closest Euclidian distance. However, this does not mean that all data will have its own pair. The reason for this is that some data will be closer to the centroid (it depends which linkage method we use) of a newly formed cluster than some separate point and this will be the next step: repeating the step one for newly formed clusters. We repeat the process until there is only one cluster. We decide on the optimal number of clusters based on the dendrogram. Linkage is disproportional to the similarity of the clusters. Therefore, we stop at the point where linkage is the largest on the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tDiscuss the various linkage methods for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Single linkage - distance between two clusters = distance between the closest points from each cluster\n",
    "\n",
    "- Complete linkage - distance between two clusters = distance between the farest points from each cluster    \n",
    "\n",
    "- Average linkage - distance between two clusters = average distance between the points from each cluster\n",
    "\n",
    "- Centroid linkage - distance between two clusters = distance between centroids from each cluster\n",
    "\n",
    "- Ward's linkage - distance between two clusters = value of variance when we combine them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tDiscuss the differences between K-Means and Hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the biggest differences between K-means and Hierarchical clustering is that in Hierarchical clustering we do not have to define the number of clusters beforehand like it is the case with K-Means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN\n",
    "19.\tDiscuss the basic terms used in DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- epsilon - distance to look for other points\n",
    "\n",
    "- minimum points - minimum number of points each cluster is required to have\n",
    "\n",
    "- core points - points that have more than minimum number of points in their epsilon range\n",
    "\n",
    "- border points - points that are not core, but are in epsilon range of some other core point\n",
    "\n",
    "- noise (outliers)- points that do not have any other point in their epsilon range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.\tDiscuss the step by step implementation of DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a point, based on the epsilon, we make a conclusion about whether it is an outlier or not. If it is, we leave it and go to the next point. If it is a core point, we find all reachable points. Also for all those points that can be classified as core, we find all reachable points. We perform previous steps for the rest of the data until we have formed the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cluster Evaluation\n",
    "21.\tWhat are the aspects of cluster validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check how well we have clustered our data, we should validate: \n",
    "\n",
    "\n",
    "- external similarity (low similarity is present between points from different clusters)\n",
    "- internal similarity (high similarity is present between nodes in the same cluster)\n",
    "- reliability (our approach is backed by statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\tWhat is a confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix is a tool for cluster validation. From it, we are able to read how many data are clustered incorrectly as well as to see the respected probabilities. These probabilities are useful for calculating the entropy and the purity of our cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\tWhat is Jaccard’s coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.\tWhat is Rand Index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25.\tWhat is the entropy of a cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in decision tree algorithms, entropy is measuring how unorganized our data is. Here, we are calculating it as a sum of logarithmic values of the probabilities that each data has not been clustered correctly. Higher the entropy, the worse our clustering is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.\tDiscuss the purity of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike entropy, purity is measuring the percentage of our data that has been clustered correctly. Higher the purity, the better our clustering algorithm is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.\tWhat are cohesion and compression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.\tWhat are the steps for AWS deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project (i'll write differences from Heroku as we have already done that):\n",
    "\n",
    "- we do not use git commands\n",
    "\n",
    "- our application should be called application.py\n",
    "\n",
    "- instead of .gitignore, we create folder .ebiignore\n",
    "\n",
    "- we need to create ebeextension folder and inside python.config file which has the following command 'option_settings:\n",
    "  \"aws:elasticbeanstalk:container:python\":\n",
    "    WSGIPath: application:application'\n",
    "    \n",
    "- zipp whole content of the project\n",
    "\n",
    "- create account on AWS page\n",
    "\n",
    "- go to option create app\n",
    "\n",
    "- fill necessary requirements\n",
    "\n",
    "- upload zipp file\n",
    "\n",
    "- creating takes time\n",
    "\n",
    "- you should receive a link for application and health bar\n",
    "\n",
    "- don't forget to terminate if you are not using \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note for myself: PycharmProjects/LinearRegressionApp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.\tWhat difficulties did you face while deploying to AWS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have only deployed one model to the AWS and did not have any issues doing so. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
