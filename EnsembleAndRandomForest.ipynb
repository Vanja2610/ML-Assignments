{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble and Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tWhat do you understand by Ensemble technique? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble technique combines prediction of one or more algorithms to come to its own. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tExplain the idea behind ensemble techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind ensamble is that the opinion of many is more important than opinion of one. By considering more than one opinion (prediction), the variance of our model is reduced and that same model is more stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat is Bootstrapping? How is sampling done in bootstrapping?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstraping is a technique of sampling. Sampling here is similar to k-fold validation with some important differences. We randomly select records from our sample, so we might select some records more than once and some records might not be selected at all. Furthermore, following this procedure, our subsamples won't be distinct as it is the case with a k-fold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat is bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is an ensemble technique that uses bootstrapping as a sampling technique. We apply the same algorithm on the subsamples and combine their prediction in order to come to the optimal one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tHow prediction is made in Bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we calculate predictions of base models. Once that is done, in the case of classification, we use voting method to determine the overall prediction and, in the case of regression, we use the average of all base predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tHow Ensemble technique solves the high variance issue with Decision trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High variance is solved by applying a Decision tree algorithm on different subsamples and combining their predictions. As said before, the prediction of many is more important than prediction of one. Also, the way we sample is very important. By randomly selecting our data and allowing overlapping, we are building more robust model with lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tWhat is pasting? How is it different from bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasting is almost the same ensemble technique as bagging. The only difference is there is no overlapping of the data. Not advisable to use it due to this reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8)\tWhat is Out Of Bag evaluation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are doing sampling with bootstrapping, we are almost never taking subsets to be the hole sample. There can be a part of the sample that is never used and that part is called Out Of Bag evaluation. We can use it for the validation of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9)\tHow does a Random Forest model works?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble technique, very similar to Bagging, where the base model is a Decision Tree. I've gave my opinion on bagging and in the next question I'll try to explain the difference between it and Random Forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10)\tWhat is the difference between Bagging and Random forest? Why do we use Random forest more commonly than Bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging is an ensemble technique that can be applied on any algorithm, while base algorithm for Random Forest is always a decision tree. Furthermore, in the base algorithm for bagging we are using all features and in decision trees of our random forest, we are using different samples of our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11)\tWhat is feature sampling? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, in random forest we do not use all features for our decision trees. Feature sampling is a procedure that occurs when we are selecting which feature we will use for our subsample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12)\tHow prediction is made in Random Forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction is made the same way as it is made with Bagging. If it is a classification problem, we use majority vote and if it is a regression problem, average is used from decision trees predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "13)\tWhen should we not use Random forest model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should not use Random forest when interpretation is an important factor or our project requires low computational time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14)\tWhat is Stacking?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking is also an ensemble technique. Here we apply different algorithms (base models) on our sample. Predictions are then used as a new sample for a new algorithm (meta model) which will give us the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "15)\tExplain the working behind Stacking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we divide our dataset into two sets, usually 50:50 (we will call it training and validation parts). After that, the first set we divide into the training and testing part. We apply our base models on the training part. When the model is complete, we find prediction using features from the validation part. Predictions are then attached to each other and we use this new dataset for training our meta model. We also find predictions of our base models, but now we use the testing part from training set. For testing our Stacking model, we use these predictions as features and dependent parts from the validation set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
