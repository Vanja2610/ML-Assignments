{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an ensemble technique and, like other ensemble techniques, we are taking into consideration of many in order to come to a better prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tHow do boosting and bagging differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main difference is that with bagging, prediction on one classifier is not influencing the prediction on others. We simply take different subsamples of our data and apply our algorithm. However, the case with boosting is that we apply a single classifier and its prediction influence how we will do the sampling for the next classifier. In boosting we are calculating contributions, but it is also possible to add weights to our bagging classifiers (I was talking about classifiers, the same approach is with regression problems)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhat are week and strong classifiers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weak and strong classifiers are classification algorithms that produce weak and strong predictions, respectively. What it means is that with weak classifiers, we are satisfied with low accuracy, while with strong classifiers we are opting for accurate models. The reason to use weak classifiers is due to their simplicity that offers some pros such as low computational time. This is one more difference between bagging and boosting, bagging uses usually strong classifiers then boosting. The reason for this is that boosting compensate this by making changes to its classifiers based on the previous ones, so we do not need strong models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhy are trees deemed fit for boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In boosting, we use stamps - the simplest possible tress. These trees fit well because: they are easy to interpret, the computational time is low, they are suitable bot for regression and classification, they deal with outliers and missing values, no need for data scalability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tExplain the step by step implementation of ADA Boost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we assign weights to our records as 1/N, where N is the sample size\n",
    "- we decide on the best stamp based on Gini impurity\n",
    "- calculate total error and from it, an amount of say or contribution of this classifier\n",
    "- next, we need to adjust the weights, highest weights to the records that were not predicted correctly. We use exponential formula for this part and once this is done, we have to normalize them sum(wi)=1\n",
    "- it is left to pick a new sample for the next stamp where the probability for the record to be picked is proportional to its weight (we can pick one record more than once)\n",
    "- we apply this procedure for every stamp in our ADA Boost algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat are pseudo residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo residuals are the difference between actual values and predicted ones. Instead of aiming for predictions, with decision trees we try to calculate these residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tExplain the step by step implementation of Gradient boosted trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boost for regression:\n",
    "\n",
    "- build a tree with a single leaf (average of the values for Y)\n",
    "- build the next tree for the pseudo residuals from the previous three (Y-average)\n",
    "- new prediction = average + learning rate * predicted pseudo residuals\n",
    "- we repeat the process when building the next trees, just instead of average, we use predictions from the previous tree\n",
    "- point of learning rate is to avoid overfitting\n",
    "- we continue the process until we reach the specified number of trees or our residuals are small enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tExplain the step by step implementation of XGBoost Algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost uses GBoost as a base. Here I will be focused on the differences between XG regressor and XGB regressor. When building a new decision tree, XGB calculates the best way we need to split our data. We do this by calculating the similarity score of the specified thresholds. In similarity score, we sum all the residuals and take into account the regularization parameter. We calculate information gain based on this similarity score. One more parameter that is important is the gamma. If the information gain is less than the gamma, we prune that tree. Trees predict our residuals and we arrive at our prediction the same way as in GB. So 3 important parameters in XGBoost are learning rate, regularization parameter and gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat are the advantages of XGBoost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- it implements regularizations\n",
    "- good with big data\n",
    "- fast algorithm (uses parallel processing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
