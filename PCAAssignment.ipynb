{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tExplain the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the distance or understanding the data, it is much harder when we have high dimensionality. That is why we need to reduce the dimension of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is a dimensionality reduction technique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction is a technique which purpose is to reduce dimensionality without losing any information from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tExplain PCA. What are the principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA we use to reduce the dimensionality of the data without losing any information from the data. We use principal components that are practically derived from the features and their purpose is to explain variance as much as possible. First principal component is a vector with direction of the highest variance of the data, next PC is in the direction of the second-highest and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat is explained variance ratio?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVR tells us how much each principal component has contributed to the explanation of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tWhat is a scree plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scree plot is a graph that shows us how much variation of our model is explained by including additional principal components. Important component of the graph is the elbow on the curved line. This shows us that after this part, an increase in a number of PCs, wont result in a high variance decrease as before.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tHow is the optimum number of principal components obtained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimum number of the principal components we can get from the scree plot. Deciding on the optimal number is a bit subjective process. We can decide how much explained variance is enough and take the minimum number that will satisfy our needs. On the other hand, we can observe the graph and see where the elbow is occurring and take the next value. It all depends on what our priorities are. For example, if the speed of the model is very important, we might sacrifice some variance explanation and take fewer PCs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat is covariance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covariance is showing us the relatioship between variables. cov(X,Y) = E(XY)-E(X)E(Y). We observe that if variables are independent, that covariance is equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is the transpose of a matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trnaspose of a matrix A is a matrix with rows eaqual to the columns of that same matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is an Eigen value and Eigen Vector?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eigen value v and eigen vector lambda for matrix A must satisfy the following equation: A * v=lambda * v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tWhy the Principal Components are orthogonal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the definition of SVD composition and due to the fact that we are dealing with the real data. Also, intuitively, they have to be orthogonal to be able to explain the highest amount of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tExplain the Eigen Decomposition approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you find the eigenvalues for the covariance matrix using the formula from linear algebra. From eigenvalues, we find corresponding eigenvectors and we choose ones that are unit length as representatives. The eigenvector with the highest eigenvalue is the PC1 and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tHow can PCA be used for data compression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tDiscuss the pros and cons of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the pros are: calculation time is decreased, we can avoid overfitting, we handle outliers, we deal with correlated data\n",
    "\n",
    "Cons: when we use PCA, we cannot interpreta our model, also, if a mistake is made, you can drop valuable information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
