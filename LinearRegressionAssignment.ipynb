{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\tWhat is Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression is a method for determining whether the relationship between independent variables and dependent variables is present or not. This method is used for predicting the values of a dependent variable using the values of independent variables. Based on the number of independent variables we have univariate (single variable) and multivariate (more than one variable) regression. This method is very popular for time series analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\tWhat is Linear Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is type of regression where we assume there is a linear relationship between independent variables and dependent one. \n",
    "General equation is: Y = a0 + a1*X1 + ... + anXn + e. \n",
    "                                n - number of independent variables,\n",
    "                                e - random error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\tWhen to use Linear Regression? Explain the equation of a straight line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use linear regression, as said above, when we assume there is a linear relationship between independent variables and dependent one, and we want to predict values of Y based on a straight line, hyperplane... This practice is common for time series - linear time series regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.\tWhat kind of plots will you use to showcase the relationship amongst the columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent the relationship between features and dependent column, I would use the first scatter plot and see if the linear relationship is present. To check the collinearity amongst the columns, I would use heatmap. By doing this, I can see if there is possible multicollinearity as well as find which independent variable has the highest linear relationship with the dependent one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.\tHow is the best fit line chosen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best fit line is chosen by measuring the distance between the line and the sample data, which means that we are measuring the distance between calculated values of Y and the actual values of Y. After doing that, we are trying to find the minimum summation of squared distances with respect to the intercept and slope of the line. The method is called gradient descent and I will explain it in a more detailed way in the next question. The intercept and slope we got from solving the problem are forming the best fit line.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.\tWhat is gradient descent, and why is it used? Explain the maths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a method used for finding optimal value oof parameters for our model by minimazing the loss function.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.\tWhat are residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residuals are differences between the calculated values of a dependent variable and its actual values. They are the part which we weren't able to describe with our model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.\tWhat is correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation is the measure of a relationship between the two variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.\tWhat is multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity is a term we use for colinearity between features (variables that are supposed to be independent). In order for our model to be reliable, our features must be independent. When training the model, we are finding the exact relationship between features and dependent variable Y. Therefore, if the relationship between some features is present as well, it will later result in inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.\tHow to detect multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE can detect multicollinearity through plots or collinearity matrix where we can spot collinearity among features. Also, we can use VIF where we are calculating how well other features explain the one feature we are testing for multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.\tWhat are the remedies for multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If multicollinearity is not that high, we can leave that variable. If multicollinearity is high, we can remove the responsible variable from the model or we can combine two variable which are colinear and make one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.\tWhat is the R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R-Squared Statistics is a measure of the goodness of our model. It tells us how much of the dataset we were able to explain with our model. We aim to be closer to 1, which means that our model was able to explain 100% of the dataset (not possible in real situations). The equation for R-Squared is as following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13.\tWhat is an adjusted R-Squared Statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusted R-Squared Statistics is another measure of how good our model is. The difference from previous R-squared is that adjusted one penalizes adding new variables to the model. The formula for calculatin it is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.\tWhy do we use adj R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use adj R-squared because it penalizes adding new variables to our model. Adding new variables can lead to increased R-Squared, but that wouldn't mean our model is better. By doing this we can make an overcomplicated model that doesn't make sense and it won't be able to predict accurately. Adj R-Squared requires that adding a new variable is justified. In other words, that contribution of the added variable is higher than the penal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.\tWhy adj R-squared decreases when we use incompetent variables?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It decreases because we are not adding value to the model, R-squared is not increasing, but by adding new variable, our penal is increased. N-p-1 part is less and nothing is increased, so adj R-squared decreases. (it doesn't have to mean R-squared has not increased, just it hasn't increased as much as it needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16.\tHow to interpret a Linear Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Linear Regression model, every coefficient represents the effect the variable, it is standing next to, has on the dependent variable. Exmpl: if dependent variable is sales and independent is marketing, then the coefficient next to marketing represents how much sales will increase if we raise one point (dollar or in whatever it is measured ) of marketing. In case the marketing is a categorical variable (1: we do market our product, 0: we do not market our product), coefficient represents how much sales will increase if we do market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17.\tWhat is the difference between fit, fit_transform and predict methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit, fit_transform and predict are methods used for different purposes. fit we use to train our model or to find optimal parameters, fit_trasform is a method used for scaling our data (usually standardizing it) and predict we use, after training and testing our data, to predict values for new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18.\tHow do you plot the least squared line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you formulate the loss function. After that, by minimizing the loss function, you get the optimal values for intercept and slope and  that is enough to plot the line: intercept + slope*feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19.\tWhat are Bias and Variance? What is Bias Variance Trade-off?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias measures how well fitted are model is and variance measures the distance between our data. Bias variance trade-off means that we have to find an optimal relation between variance and bias, so our total error is minimum. With increasing the complexity of the model, we lower the bias but increase the variance. If we are trying to match our model too much to the test data, we will have a lower bias, but or model won't work for test data which will result in high variance (overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.\tWhat is the null and alternate hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics when we want to test some statements, we use null and alternate hypotheses. Usually, in the null (H0) hypothesis, we state that something is true and with alternate (H1) we try to refute the statement. For example, in linear regression H0: beta=0 and with H1, based on the sample, we try to prove that is not true. If we are successful, we say H1 is true (it doesn't have to be, but our sample said that), and if not, that H0 holds. All in all, we add variables to our model if the alternative hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21.\tWhat is multiple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regrssion is linear regression where we have multiple features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22.\tWhat is the OLS method? Derive the formulae used in the OLS method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS is a method used for minimizing the loss function or minimizing the sum of squared residuals with respect to intercept and slope. By doing so, we are able to find optimal parameters for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23.\tWhat is the p-value? How does it help in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value is the area under the distribution curve with left border t-registered (for testing the significance of our feature, we use t statistics). If p>alfa (which is usually 0.05), we do not reject the null hypothesis, or the corresponding feature is not significant. In other words, we add a new feature to the model if t-statistics give us small p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24.\tHow to handle categorical values in the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We handle categorical variables with the dummy variables. For example: if we have a variable with the values good and bad, we can map it as good: 1 and bad: 0. In case the variable has more than one category, we use multiple dummy variables. Example: values are: man, woman and child. We can map man:11, woman:00 and child: 10. We do not need as many dummy variables as we have values. There is a possibility that we have a variable which we can be ranked. Exmpl: small, average, large and we could maybe map it as 1, 2 and 3, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25.\tWhat is regularization, and why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a method used for decreasing the variance of our model. We need it because if the high variance is present, our predictions won't be accurate. Basically, the method adds punishment for big slopes in calculating the total error (total error = summation of squared residuals + punishment). The reason behind this is that we are lowering the impact of some features. By doing this, we are sacrificing some bias for a lower variance. Our model will be more stable. Lambda is a parameter we use that tells us how much we would like our slope to have significance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26.\tExplain Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge Regression is a regularization method where penalty part = lambda*sum(slpoes^2). We use this method where there are no insignificant features in our model or when there is no correlation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.\tExplain Lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization method where penalty part = lambda*sum(|slopes|). We use this method when we know that our features are highly correlated. The reason for this is that in Lasso regression we can make the slope of some variables go all the way to zero by increasing lambda, while that is not possible in Ridge (only close to zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.\tExplain Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic Net is a combination of Ridge and Lasso regression, penalty part = lambda1*sum(slopes^2)+lambda2*sum(|slope|). We use them when we do not know whether the correlation is present among our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.\tWhy do we do a train test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a train test split in order to catch overfitting or in other words to see if our model has really learned or it only works for one particular dataset. It is not enough to just train our model, we have to see how it will work on a new dataset and that will actually give us an understanding of how good it is. To summarize, train part we use for building a model and test part for evaluating the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30.\tWhat is polynomial regression? When to use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression is a method where for predicting dependent variable Y, we use a polynom of independent variables Xi. The purpose of this method is to describe the nonlinear relationship between features and Y. In other words, if we conclude that we are not able to describe the relationship with a line (for univariate regression), we can turn to polynomial regression, where instead of a line, we will use a curve. Polynomial regression is just one of the algorithms for this purpose.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.\tExplain the steps for GCP deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to being from EU country, I was unable to sign in as an individual (only option was to sign as a business). Therefore, I was not able to create a google cloud account. I have confirmed with Mr. Sudanshu that it is fine to skip this part and do the deployment on other platforms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "32. What difficulties did you face in cloud deployment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same answer as above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
