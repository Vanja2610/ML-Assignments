{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tExplain the working of KNN algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In KNN algorithm we are calculating the distances between records. First we need to find the optimal value for k and once that is done, we find k nearest nodes (records) to our data. Based on them and whether we are talking about classification or regression problem, we make a respective decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tHow KNN predicts the output for regression and classification problems?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When talking about the regression problem, for prediction, KNN takes an average of N nearest nodes and in the case of classification, we go with the voting approach. For example, if k = 3 and 2 nodes belong to class A and 1 to class B, we will say that the predicted class is class A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat are the different distances used in KNN? How are they calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In KNN, we can use the following distances betewwn two nodes x=(x1,x2,...,xn) and y=(y1,y2,...,yn):\n",
    "\n",
    "1) Euklidian distance (most common one) - distance = sqrt((x1-y1)^2+...(xn-yn)^2)\n",
    "\n",
    "2) L1 distance - distance = |x1-y1|+...+|xn-yn|\n",
    "\n",
    "3) Hamming distance (for categorical data) - distance = sum(I(where xi is not equal to yi)). Example: x = (a,b,c), y =(a,d,b), distance will be 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat are Lazy Learners? Why KNN is called a lazy learner?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN is a lazy learner because we are not building a model here like it was the case with the previous algorithms. Every time we are predicting class or exact value, we are calculating the distance between new data and the whole sample. We are not using an already built model and just applying it to the additional data we get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tHow do we select the value of k? How bias and variance varies with k?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to select the optimal value of k, we need to find the optimal variance-bias rate. With the increase of k, our variance is decreasing and bias increasing. Therefore if we select k that is not large enough, it can come to an underfitting and for larger values, we might get wrong predictions. We can decide on the perfect value by drawing a graph where variance and bias depend on the k and find the optimal trade. Another way is to use a k-fold validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tWhat are advantages and disadvantages of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to being a Lazy learner, KNN is not good with big data logically. Also, due to the need for storage of the whole dataset, it needs a lot of space. On the other hand, KNN is very easy to understand and implement. Furthermore, we can use it both for regression and classification. In my opinion, knn is a very good method when used with other algorithms such as clustering algorithms or as a base model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tDiscuss kDTree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, KNN is not good for big datasets because it is a Lazy learner. kDtree algorithm we use to deal with this issue. We separate our data based on the features and build a tree. So, when we get new data, we just go through the tree and save computational time. For example if we have the following features: X=(a,b,c), Y =(1,2,3), Z is continuous variable and we get new data W=(c,2,145.32). What we need to do is to go through the built tree, find brunches where X=c, Y=2, and Z is in that interval. We apply knn on the nodes in that leaf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "8)\tDiscuss Ball Tree algorithm used for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ball Tree algorithm, as kDTree, we use to deal with the disadvantage of KNN being a Lazy learner. We use clustering to make clusters of our sample before calculating the KNN. This way, we only need to find which cluster the new data will belong to and calculate the distance only between that data and subsample in that specific cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
