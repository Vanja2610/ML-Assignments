{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tHow decision tree works for a regression problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are using DT for Regression, we have to divide our features into batches. Thresholds for division are calculated based on the sum of squared residuals (sum(r_i) where r_i = sum((y_i-avg(y_i))^2). Basically, we are finding the average value of dependent variable for each batch and calculating the distance from other values. We do that for every batch and we find the minimum of the corresponding sum with respect to the threshold. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tWhy Recursive binary splitting is called Greedy Approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat do you understand by Greedy approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the Greedy approach, I think we consider a method where we get descent models, but maybe not optimal in purpose for decreasing computational power and time. CART is a greedy algorithm. We do not consider whether we should split some later nodes in the tree (first ones are optimal), which saves us power but can increase variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4)\tWhat is Pruning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning is a method for lowering the complexity of our decision tree or, in other words, cutting down some branches. If our tree is having very high number of branches, we might want to decrease it. The reason for this might be in decreasing the computation power of our model or avoiding overfitting of our model. Pruning is done by setting parameters or conditions needed for splitting. One example of pruning is setting the minimum number of data each leaf node needs to have in the regression decision tree (the reason for this is to decrease the possibility for overfitting).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5)\tWhat’s the difference between pre pruning and post pruning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre pruning is pruning done before creating the model (we set hyperparameters), while post pruning is pruning that occurs after we have created the model and we want to decrease the number of branches in our model (maybe we have realized our model is overfitted). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6)\tWhat is Entropy? How is it calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy measures how much or data is disordered. More stable the features is, it will give more information. Therefore, the less the Entropy is, the higher the Information Gain is for the specific node. Formula for entropy: sum(-p_i*log_2(p_i))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7)\tWhat is Gini Impurity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gini Impurity measures how wrongly we have divided the node (less the Gini, we have separated our node better). Formula for Gini Impurity: 1-sum(p_i^2), where p_i is the probability for each class. In case we have perfectly separated data, one of our p_i would be equal to 1 and Gini would be 0. Formula punishes if our probability is less than one, which will result in Gini higher than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8)\tWhat do you understand by Information Gain? How does it help in tree building?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information Gain, as the name says, tell us how much more information we have gained by splitting a certain node or how much entropy of the whole data has been decreased. From Information Gain we are able to see how much splitting certain node has contributed to explaining a certain class. If Information gain is equal to 1, it means that we have split data perfectly, while, if it is 0, we have not contributed to the understanding of our dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9)\tHow does node selection take place while building a tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide on a node by how much information gain we will get by splitting it. Node with the highest Information Gain or the lowest Gini Impurity will be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10)\tWhat are different algorithms available for decision tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms:\n",
    "\n",
    "1. ID 3 - uses information gain\n",
    "\n",
    "2. C 4.5 - advance version of ID3, can handle continuous data, handle missing values, also uses information gain\n",
    "\n",
    "3. CART - default is Gini, but you can set to use information gain, can handle both regression and classification (previous               algorithms could handle only classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11)\tWhat’s the main difference between Gini Impurity and Entropy on the basis of computation time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entropy's formula uses log, while Gini only uses summation and both calculate probability. Therefore, I would say that the computation time for Entropy is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12)\tWhat are the disadvantages and advantages of using a Decision Tree?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages:\n",
    "\n",
    "- Easy to understand. Algorithm is quite logical and we can see how nodes are split.\n",
    "- We can use it both for regression and classification\n",
    "- It works well with complex data. Data we could not explain with linear and logistic regression can maybe be explained with DT.\n",
    "- Later with ensemble techniques, it can give us powerful models.\n",
    "- It handles missing scaling\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "- It can easily come to overfitting. We should always consider pruning.\n",
    "- Computation time is higher than other classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13)\tHow do you deploy model in Heroku?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have answered this question in the Logistic regression assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14)\tWhat challenges you faced while deploying the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same answer as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)\tWhat is Cross validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation is a method we use to lower the variance of our data or to avoid overfitted models. The procedure of this method is to use subsamples of our dataset and test the model on each subset in order to get a better understanding of how good our model really is. Cross validation is usually used when we are comparing different algorithms or deciding which hyperparameter to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)\tWhy do we need to implement Cross validation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said above, cross validation is used to lower the variance of our model. We need to use it because if our model works fine just for one sample, it doesn't mean that it will work as well on a new one. That is why we split the whole dataset into more than one subset. This way we make sure our model is not overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3)\tWhat are different types of CV methods?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First one is the Hold Out method where we divide our dataset into the train and test part. The second one is k-fold Validation. Here we also divide our data into train and test, but this time we do it k times differently. A special case of k-fold is when k=n (sample size) and we have 1 record for testing. This type is called LOOCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "4)\tHow bias and variance varies for each CV method?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important thing is that for increasing data size for training, we are lowering bias, but increasing variance. We are risking that our model becomes overfitted. Therefore, LOOCV has lower bias and higher variance than regular k-fold method. Also, that is the case with a hold out method. As said before, k-fold method is useful because for sacrificing a little bias (small amount of bias increases), we are decreasing variance or in other words, we are getting a more stable model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "5)\tIs Train Test Split a kind of CV? True or False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True - Hold Out method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "6)\tHow can we check over fitting using CV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A signal for overfitted model is when our model performs much better on  training than on  test data. Also, in k fold validation, if it is performing well for certain test data and not so much for others.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
